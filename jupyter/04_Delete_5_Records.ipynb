{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗑️ Delete 5 Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delete5Records\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.es.nodes\", \"elasticsearch\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.14.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_record_by_id(doc_id):\n",
    "    \"\"\"Delete a record from Elasticsearch by ID using HTTP DELETE\"\"\"\n",
    "    url = f\"http://elasticsearch:9200/2_people_data_2k_spark/_doc/{doc_id}\"\n",
    "    try:\n",
    "        response = requests.delete(url)\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting record {doc_id}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs to delete\n",
    "delete_ids = [24410114, 24410100, 24410109, 24410092, 24410040]\n",
    "print(f\"Deleting records with IDs: {delete_ids}\")\n",
    "\n",
    "# Read current data to show records before deletion\n",
    "current_df = spark.read \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.resource\", \"2_people_data_2k_spark\") \\\n",
    "    .load()\n",
    "\n",
    "# Show records before deletion\n",
    "records_to_delete = current_df.filter(col(\"id\").isin(delete_ids))\n",
    "print(f\"Records to delete ({records_to_delete.count()}):\")\n",
    "records_to_delete.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete records using HTTP DELETE\n",
    "print(\"Deleting records...\")\n",
    "deleted_count = 0\n",
    "for doc_id in delete_ids:\n",
    "    if delete_record_by_id(doc_id):\n",
    "        print(f\"✓ Deleted record ID: {doc_id}\")\n",
    "        deleted_count += 1\n",
    "    else:\n",
    "        print(f\"✗ Failed to delete record ID: {doc_id}\")\n",
    "\n",
    "print(f\"Deleted {deleted_count} out of {len(delete_ids)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify deletion\n",
    "print(\"Verifying deletion...\")\n",
    "updated_df = spark.read \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.resource\", \"2_people_data_2k_spark\") \\\n",
    "    .load()\n",
    "\n",
    "remaining_records = updated_df.filter(col(\"id\").isin(delete_ids))\n",
    "remaining_count = remaining_records.count()\n",
    "total_count = updated_df.count()\n",
    "\n",
    "print(f\"Remaining records with deleted IDs: {remaining_count}\")\n",
    "print(f\"Total records after deletion: {total_count}\")\n",
    "\n",
    "if remaining_count > 0:\n",
    "    print(\"Remaining records:\")\n",
    "    remaining_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

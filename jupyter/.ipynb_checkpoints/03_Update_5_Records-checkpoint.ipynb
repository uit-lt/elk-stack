{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Update 5 Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application ID: local-1757837541431\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Update5Records\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.es.nodes\", \"elasticsearch\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.14.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating records with IDs: [24410114, 24410100, 24410109, 24410092, 24410040]\n"
     ]
    }
   ],
   "source": [
    "# Define updated records data\n",
    "updated_records_data = [\n",
    "    (24410114, \"Tran Trieu Thuan updated\", 30),\n",
    "    (24410100, \"Nguyen Phuong Tan updated\", 30),\n",
    "    (24410109, \"Nguyen Thi Thu Thao updated\", 28),\n",
    "    (24410092, \"Huynh Duy Quoc updated\", 35),\n",
    "    (24410040, \"Ha Huy Hung updated\", 22)\n",
    "]\n",
    "\n",
    "update_ids = [record[0] for record in updated_records_data]\n",
    "print(f\"Updating records with IDs: {update_ids}\")\n",
    "\n",
    "# Read current data\n",
    "current_df = spark.read \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.resource\", \"2_people_data_2k\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before update:\n",
      "+----------+--------+---+-----+--------+----+--------+----+-------+-------------------+\n",
      "|@timestamp|@version|age|event|filename|host|      id| log|message|               name|\n",
      "+----------+--------+---+-----+--------+----+--------+----+-------+-------------------+\n",
      "|      NULL|    NULL| 28| NULL|    NULL|NULL|24410109|NULL|   NULL|Nguyen Thi Thu Thao|\n",
      "|      NULL|    NULL| 30| NULL|    NULL|NULL|24410100|NULL|   NULL|  Nguyen Phuong Tan|\n",
      "|      NULL|    NULL| 30| NULL|    NULL|NULL|24410114|NULL|   NULL|   Tran Trieu Thuan|\n",
      "|      NULL|    NULL| 35| NULL|    NULL|NULL|24410092|NULL|   NULL|     Huynh Duy Quoc|\n",
      "|      NULL|    NULL| 22| NULL|    NULL|NULL|24410040|NULL|   NULL|        Ha Huy Hung|\n",
      "+----------+--------+---+-----+--------+----+--------+----+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show records before update\n",
    "records_before_update = current_df.filter(col(\"id\").isin(update_ids))\n",
    "print(\"Records before update:\")\n",
    "records_before_update.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after update:\n",
      "+--------+--------------------+---+\n",
      "|      id|                name|age|\n",
      "+--------+--------------------+---+\n",
      "|24410114|Tran Trieu Thuan ...| 30|\n",
      "|24410100|Nguyen Phuong Tan...| 30|\n",
      "|24410109|Nguyen Thi Thu Th...| 28|\n",
      "|24410092|Huynh Duy Quoc up...| 35|\n",
      "|24410040| Ha Huy Hung updated| 22|\n",
      "+--------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with updated data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "updated_df = spark.createDataFrame(updated_records_data, schema)\n",
    "\n",
    "print(\"Records after update:\")\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing updated records to Elasticsearch...\n",
      "âœ“ Records updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Write updated records back to Elasticsearch\n",
    "print(\"Writing updated records to Elasticsearch...\")\n",
    "try:\n",
    "    updated_df.write \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "        .option(\"es.port\", \"9200\") \\\n",
    "        .option(\"es.resource\", \"2_people_data_2k\") \\\n",
    "        .option(\"es.mapping.id\", \"id\") \\\n",
    "        .option(\"es.batch.size.bytes\", \"10mb\") \\\n",
    "        .option(\"es.batch.size.entries\", \"1000\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    print(\"âœ“ Records updated successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Update failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify update\n",
    "print(\"\\nVerifying update...\")\n",
    "# Read fresh data to verify the update\n",
    "verification_df = spark.read \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "    .option(\"es.port\", \"9200\") \\\n",
    "    .option(\"es.resource\", \"2_people_data_2k\") \\\n",
    "    .load()\n",
    "\n",
    "updated_records = verification_df.filter(col(\"id\").isin(update_ids))\n",
    "updated_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
